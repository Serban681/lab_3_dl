{"cells":[{"cell_type":"markdown","id":"873828f0-e50f-40b9-9879-f9a01adaa020","metadata":{"id":"873828f0-e50f-40b9-9879-f9a01adaa020","tags":[]},"source":["# Lab 3: PyTorch big project\n","\n","# Part 2: PyTorch Paper Replicating\n","\n","In this part, we're going to be **replicating a machine learning research paper** and creating a Vision Transformer (ViT) from scratch using PyTorch.\n","\n","We'll then see how ViT, a state-of-the-art computer vision architecture, performs on our FoodVision Mini problem.\n"]},{"cell_type":"markdown","id":"b09a7ccf-41e8-4ee4-8d78-aff5f650ca7f","metadata":{"id":"b09a7ccf-41e8-4ee4-8d78-aff5f650ca7f"},"source":["## Where can you find code examples for machine learning research papers?\n","\n","One of the first things you'll notice when it comes to machine learning research is: there's a lot of it. There are several places to find and read machine learning research papers (and code):\n","\n","| **Resource** | **What is it?** |\n","| ----- | ----- |\n","| [arXiv](https://arxiv.org/) | Free and open resource for reading technical articles on everything from physics to computer science (including machine learning). |\n","| [AK Twitter](https://twitter.com/_akhaliq) | The AK Twitter account publishes machine learning research highlights, often with live demos almost every day.  |\n","| [Papers with Code](https://paperswithcode.com/) | A curated collection of machine learning papers, many of which include code resources attached. |\n","| [lucidrains' `vit-pytorch` GitHub repository](https://github.com/lucidrains/vit-pytorch) | Example of what paper replicating with code on a larger-scale and with a specific focus looks like. The `vit-pytorch` repository is a collection of Vision Transformer model architectures from various research papers replicated with PyTorch code. |"]},{"cell_type":"markdown","id":"329412b2-fb66-46de-ba72-2aa5092eb231","metadata":{"id":"329412b2-fb66-46de-ba72-2aa5092eb231"},"source":["## What we're going to cover\n","\n","We're going to be replicating the machine learning research paper [*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*](https://arxiv.org/abs/2010.11929)  (ViT paper) with PyTorch.\n","\n","The Transformer neural network architecture was originally introduced in the machine learning research paper [*Attention is all you need*](https://arxiv.org/abs/1706.03762).\n","\n","And the original Transformer architecture was designed to work on one-dimensional (1D) sequences of text. Like the name suggests, **the Vision Transformer (ViT) architecture was designed to adapt the original Transformer architecture to vision problem(s)**."]},{"cell_type":"markdown","id":"7a8913de-e49e-40c9-89c7-6b847fac9def","metadata":{"id":"7a8913de-e49e-40c9-89c7-6b847fac9def"},"source":["## 0. Getting setup\n","\n","Let's start by downloading all of the modules we'll need for this section. `torchinfo` will help later on to give us visual summaries of our model(s)."]},{"cell_type":"code","source":["# If you are using Google Colab\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd '/content/drive/My Drive/Laboratory 03/'"],"metadata":{"id":"lkDe0Q7PqA7N"},"id":"lkDe0Q7PqA7N","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ebe46d77-6c4d-4102-9994-2cb89f633f18","metadata":{"id":"ebe46d77-6c4d-4102-9994-2cb89f633f18"},"outputs":[],"source":["# For this notebook to run with updated APIs, we need torch 2.0+ and torchvision 0.15+\n","try:\n","    import torch\n","    import torchvision\n","    assert int(torch.__version__.split(\".\")[0]) >= 2, \"torch version should be 2.0+\"\n","    assert int(torchvision.__version__.split(\".\")[1]) >= 15, \"torchvision version should be 0.15+\"\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")\n","except:\n","    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n","    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n","    import torch\n","    import torchvision\n","    print(f\"torch version: {torch.__version__}\")\n","    print(f\"torchvision version: {torchvision.__version__}\")"]},{"cell_type":"markdown","id":"30caf875-557e-410f-8dff-bd4a9f6c7ae4","metadata":{"id":"30caf875-557e-410f-8dff-bd4a9f6c7ae4"},"source":["> **Note:** If you're using Google Colab, you may have to restart your runtime after running the above cell. After restarting, you can run the cell again and verify you've got the right versions of `torch` (2.0+) and `torchvision` (0.15+).\n"]},{"cell_type":"code","execution_count":null,"id":"960eb156-c1b1-4e76-a812-01bf045835bd","metadata":{"id":"960eb156-c1b1-4e76-a812-01bf045835bd"},"outputs":[],"source":["# Continue with regular imports\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","import utils"]},{"cell_type":"markdown","id":"4f9bdd26-26ac-4756-bd8e-b7a50799f28b","metadata":{"id":"4f9bdd26-26ac-4756-bd8e-b7a50799f28b"},"source":["> **Note:** If you're using Google Colab, and you don't have a GPU turned on yet, it's now time to turn one on via `Runtime -> Change runtime type -> Hardware accelerator -> GPU`."]},{"cell_type":"code","execution_count":null,"id":"5e246f92-e509-474e-b6c7-c82cf11cb8ca","metadata":{"id":"5e246f92-e509-474e-b6c7-c82cf11cb8ca"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","id":"5a695192-2644-4aa7-beab-7222a24b1a1a","metadata":{"id":"5a695192-2644-4aa7-beab-7222a24b1a1a"},"source":["## 1. Get Data\n"]},{"cell_type":"code","execution_count":null,"id":"37b5ffc0-7093-481e-8081-dbdfac4c24f0","metadata":{"id":"37b5ffc0-7093-481e-8081-dbdfac4c24f0"},"outputs":[],"source":["image_path = utils.lab3.download_data(destination=\"pizza_steak_sushi\")\n","image_path"]},{"cell_type":"code","execution_count":null,"id":"92a426b6-df22-4a58-9d5e-b382c73c6048","metadata":{"id":"92a426b6-df22-4a58-9d5e-b382c73c6048"},"outputs":[],"source":["# Setup directory paths to train and test images\n","train_dir = image_path / \"train\"\n","test_dir = image_path / \"test\""]},{"cell_type":"markdown","id":"d6fce58f-0f0b-48ef-a9b6-1ef3ca09380d","metadata":{"id":"d6fce58f-0f0b-48ef-a9b6-1ef3ca09380d"},"source":["## 2. Create Datasets and DataLoaders\n","\n","In Table 3, the training resolution is mentioned as being 224 (height=224, width=224).\n","\n","<img src=\"https://docs.google.com/uc?id=1LdV2s16l0OoeQZdtBeAvYQPzO6Yj0LCK\" width=900 alt=\"Table 3 from the Vision Transformer paper showing the image size and batch size\"/>\n","\n","*You can often find various hyperparameter settings listed in a table. In this case we're still preparing our data, so we're mainly concerned with things like image size and batch size. Source: Table 3 in [ViT paper](https://arxiv.org/abs/2010.11929).*\n","\n","So we'll make sure our transform resizes our images appropriately. And since we'll be training our model from scratch (no transfer learning to begin with), we won't provide a `normalize` transform like we did in Part 1.\n","\n","### 2.1 Prepare transforms for images"]},{"cell_type":"code","execution_count":null,"id":"a45ea650-c3fa-479c-8767-48bc3a1f1267","metadata":{"id":"a45ea650-c3fa-479c-8767-48bc3a1f1267"},"outputs":[],"source":["IMG_SIZE = 224\n","\n","# Create transform pipeline manually\n","manual_transforms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","])\n","print(f\"Manually created transforms: {manual_transforms}\")"]},{"cell_type":"markdown","id":"437078c2-eb42-471f-8561-94845d0a878d","metadata":{"id":"437078c2-eb42-471f-8561-94845d0a878d"},"source":["### 2.2 Turn images into `DataLoader`'s\n","The ViT paper states the use of a batch size of 4096. However, we're going to stick with a batch size of 32.\n","\n","An extension of this project could be to try a higher batch size value and see what happens.\n","\n","> **Note:** We're using the `pin_memory=True` parameter in the `create_dataloaders()` function to speed up computation. `pin_memory=True` avoids unnecessary copying of memory between the CPU and GPU memory by \"pinning\" examples that have been seen before. Though the benefits of this will likely be seen with larger dataset sizes (our FoodVision Mini dataset is quite small). However, setting `pin_memory=True` doesn't *always* improve performance (this is another one of those we're scenarios in machine learning where some things work sometimes and don't other times). See the PyTorch [`torch.utils.data.DataLoader` documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."]},{"cell_type":"code","execution_count":null,"id":"d0ac8145-f89a-490f-82e3-d4b22225d163","metadata":{"id":"d0ac8145-f89a-490f-82e3-d4b22225d163"},"outputs":[],"source":["# Set the batch size\n","BATCH_SIZE = 32\n","\n","# Create data loaders\n","train_dataloader, test_dataloader, class_names = utils.lab3.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    transform=manual_transforms, # use manually created transforms\n","    batch_size=BATCH_SIZE\n",")\n","\n","train_dataloader, test_dataloader, class_names"]},{"cell_type":"code","execution_count":null,"id":"b5734a22-ded5-403e-84f5-d7a90ed3f085","metadata":{"id":"b5734a22-ded5-403e-84f5-d7a90ed3f085"},"outputs":[],"source":["# Get a batch of images\n","image_batch, label_batch = next(iter(train_dataloader))\n","\n","# Get a single image from the batch\n","image, label = image_batch[0], label_batch[0]\n","\n","# View the batch shapes\n","image.shape, label"]},{"cell_type":"code","execution_count":null,"id":"afe85fae-38fd-4f34-a52c-29d02cce09c1","metadata":{"id":"afe85fae-38fd-4f34-a52c-29d02cce09c1"},"outputs":[],"source":["# Plot image with matplotlib\n","plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n","plt.title(class_names[label])\n","plt.axis(False);"]},{"cell_type":"markdown","id":"c2852f3f-61f0-4dad-ae8c-49db54e28470","metadata":{"id":"c2852f3f-61f0-4dad-ae8c-49db54e28470"},"source":["## 3. Replicating the ViT paper\n","ViT is a deep learning neural network architecture. For a better understanding, we'll break it down, starting with the inputs and outputs of single layer and working up to the inputs and outputs of the whole model.\n","\n","There are many little details about the ViT model sprinkled throughout the paper. The main three resources we'll be looking at for the architecture design are:\n","1. **Figure 1** - An overview of the model in a graphical sense, you could *almost* recreate the architecture with this figure alone.\n","2. **Four equations in section 3.1** - These equations give a little bit more of a mathematical grounding to the coloured blocks in Figure 1.\n","3. **Table 1** - Various hyperparameter settings (such as number of layers and number of hidden units) for different ViT model variants. We'll be focused on the smallest version, ViT-Base."]},{"cell_type":"markdown","id":"4c90c4c0-0039-4790-a25b-f14754e6b468","metadata":{"id":"4c90c4c0-0039-4790-a25b-f14754e6b468"},"source":["#### 3.1 Exploring Figure 1\n","\n","The main things we'll be paying attention to are:\n","1. **Layers** - takes an **input**, performs an operation or function on the input, produces an **output**.\n","2. **Blocks** - a collection of layers, which in turn also takes an **input** and produces an **output**.\n","\n","<img src=\"https://docs.google.com/uc?id=1DgCr2q4XF8MSpIOmSA_TuWdXqmU_53GP\" width=900 alt=\"figure 1 from the original vision transformer paper\"/>\n","\n","*Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture.*\n","\n","The ViT architecture is comprised of several stages:\n","* **Patch + Position Embedding (inputs)** - Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.\n","* **Linear projection of flattened patches (Embedded Patches)** - The image patches get turned into an **embedding**.\n","* **Norm** - \"[Layer Normalization](https://paperswithcode.com/method/layer-normalization)\", you can use LayerNorm via the PyTorch layer [`torch.nn.LayerNorm()`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n","* **Multi-Head Attention** - This is a [Multi-Headed Self-Attention layer](https://paperswithcode.com/method/multi-head-attention) or \"MSA\" for short. You can create an MSA layer via the PyTorch layer [`torch.nn.MultiheadAttention()`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html).\n","* **MLP (or [Multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron))** - In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two [`torch.nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers with a [`torch.nn.GELU()`](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) non-linearity activation in between them (section 3.1) and a [`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layer after each (Appendix B.1).\n","* **Transformer Encoder** - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the \"+\" symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.\n","* **MLP Head** - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block.\n","\n","Let's take Figure 1 and adapt it to our FoodVision Mini problem of classifying images of food into pizza, steak or sushi.\n","\n","<img src=\"https://docs.google.com/uc?id=1W_kREtrX-T6NWx751YtrOvHxQOpi-mX1\" width=900 alt=\"figure 1 from the original vision transformer paper adapted to work with food images, an image of pizza goes in and gets classified as 'pizza'\"/>\n","\n","*Figure 1 from the ViT Paper adapted for use with FoodVision Mini. An image of food goes in (pizza), the image gets turned into patches and then projected to an embedding. The embedding then travels through the various layers and blocks and (hopefully) the class \"pizza\" is returned.*"]},{"cell_type":"markdown","id":"d95ff18c-9df1-45fa-a22d-9eca72160b6e","metadata":{"id":"d95ff18c-9df1-45fa-a22d-9eca72160b6e"},"source":["#### 3.2 Exploring the Four Equations\n","\n","The next main part(s) of the ViT paper we're going to look at are the four equations in section 3.1.\n","\n","<img src=\"https://docs.google.com/uc?id=1kO_z10b-4slgpKY6JosS3SKok4Upv5nz\" width=650 alt=\"four mathematical equations from the vision transformer machine learning paper\"/>\n","\n","*These four equations represent the math behind the four major parts of the ViT architecture.*\n","\n","| **Equation number** | **Description from ViT paper section 3.1** |\n","| ----- | ----- |\n","| 1 | The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a **trainable linear projection** (Eq. 1). We refer to the output of this projection as the **patch embeddings**... **Position embeddings** are added to the patch embeddings to retain positional information. We use standard **learnable 1D position embeddings**...|\n","| 2 | The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). **Layernorm (LN) is applied before every block**, and **residual connections after every block** (Wang et al., 2019; Baevski & Auli, 2019). |\n","| 3 | Same as equation 2. |\n","| 4 | Similar to BERT's [ class ] token, we **prepend a learnable embedding to the sequence of embedded patches** $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4)... |"]},{"cell_type":"markdown","id":"cd36899e-5bc7-411a-aab7-28e3a5a2c6cb","metadata":{"id":"cd36899e-5bc7-411a-aab7-28e3a5a2c6cb"},"source":["#### 3.3 Exploring Table 1\n","\n","The final piece of the ViT architecture puzzle we'll focus on is Table 1.\n","\n","| Model | Layers | Hidden size $D$ | MLP size | Heads | Params |\n","| :--- | :---: | :---: | :---: | :---: | :---: |\n","| ViT-Base | 12 | 768 | 3072 | 12 | $86M$ |\n","| ViT-Large | 24 | 1024 | 4096 | 16 | $307M$ |\n","| ViT-Huge | 32 | 1280 | 5120 | 16 | $632M$ |\n","\n","<div align=left>\n","    <i>Table 1: Details of Vision Transformer model variants. Source: <a href=\"https://arxiv.org/abs/2010.11929\">ViT paper</a>.</i>\n","</div>\n","<br>\n","\n","We're going to focus on replicating ViT-Base (start small and scale up when necessary) but we'll be writing code that could easily scale up to the larger variants.\n","\n","Breaking the hyperparameters down:\n","* **Layers** - How many Transformer Encoder blocks are there? (each of these will contain a MSA block and MLP block)\n","* **Hidden size $D$** - This is the embedding dimension throughout the architecture, this will be the size of the vector that our image gets turned into when it gets patched and embedded. Generally, the larger the embedding dimension, the more information can be captured, the better results. However, a larger embedding comes at the cost of more computation.\n","* **MLP size** - What are the number of hidden units in the MLP layers?\n","* **Heads** - How many heads are there in the Multi-Head Attention layers?\n","* **Params** - What are the total number of parameters of the model? Generally, more parameters leads to better performance but at the cost of more computation. You'll notice even ViT-Base has far more parameters than any other model we've used so far.\n","\n","We'll use these values as the hyperparameter settings for our ViT architecture."]},{"cell_type":"markdown","id":"9f1717f5-f6bc-4cce-b5eb-093822da988d","metadata":{"id":"9f1717f5-f6bc-4cce-b5eb-093822da988d","tags":[]},"source":["## 4. Equation 1: Split data into patches and creating the class, position and patch embedding\n","\n","If you can represent your data in a good, learnable way (as **embeddings are learnable representations**), a learning algorithm will be able to perform well on them. Let's start by creating the class, position and patch embeddings for the ViT architecture.\n","\n","We'll start with the **patch embedding**.\n","\n","This means we'll be turning our input images in a sequence of patches and then embedding those patches. We'll begin by following the opening paragraph of section 3.1 of the ViT paper:\n","\n","> The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the **patch embeddings**.\n","\n","And size we're dealing with image shapes, let's keep in mind the line from Table 3 of the ViT paper:\n","\n","> Training resolution is **224**.\n","\n","Let's break down the text above.\n","\n","* $D$ is the size of the **patch embeddings**, different values for $D$ for various sized ViT models can be found in Table 1.\n","* The image starts as 2D with size ${H \\times W \\times C}$.\n","    * $(H, W)$ is the resolution of the original image (height, width).\n","    * $C$ is the number of channels.\n","* The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.\n","    * $(P, P)$ is the resolution of each image patch (**patch size**).\n","    * $N=H W / P^{2}$ is the resulting number of patches, which also serves as the input sequence length for the Transformer.\n","\n","<img src=\"https://docs.google.com/uc?id=1ncR5FZSjRdVSAWGBceJR9AslYrlKhQob\" width=900 alt=\"mapping the vit architecture diagram positional and patch embeddings portion to the relative mathematical equation describing what's going on\"/>\n","\n","*Mapping the patch and position embedding portion of the ViT architecture from Figure 1 to Equation 1. The opening paragraph of section 3.1 describes the different input and output shapes of the patch embedding layer.*"]},{"cell_type":"markdown","id":"2010c168-88c7-4045-8c02-8e759ffacef8","metadata":{"id":"2010c168-88c7-4045-8c02-8e759ffacef8","tags":[]},"source":["### 4.1 Calculating patch embedding input and output shapes by hand\n","\n","Create some variables to mimic each of the terms (such as $H$, $W$ etc) above. Use a patch size ($P$) of 16 since it's the best performing version of ViT-Base uses (see column \"ViT-B/16\" of Table 5 in the ViT paper for more)."]},{"cell_type":"code","execution_count":null,"id":"bb10d7f1-1aca-416f-b5a4-3abe722ff207","metadata":{"id":"bb10d7f1-1aca-416f-b5a4-3abe722ff207"},"outputs":[],"source":["# Create example values\n","height = 224 # H (\"The training resolution is 224.\")\n","width = 224 # W\n","color_channels = 3 # C\n","patch_size = 16 # P\n","\n","# Calculate N (number of patches)\n","number_of_patches = int((height * width) / patch_size**2)\n","print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"]},{"cell_type":"markdown","id":"0e5f118e-e828-498a-abe1-0de8a5d90cd5","metadata":{"id":"0e5f118e-e828-498a-abe1-0de8a5d90cd5"},"source":["Replicate the input and output shapes of the patch embedding layer.\n","\n","Recall:\n","\n","* **Input:** The image starts as 2D with size ${H \\times W \\times C}$.\n","* **Output:** The image gets converted to a sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$."]},{"cell_type":"code","execution_count":null,"id":"1f684bab-1e4e-4251-99b7-839b0b69dbd3","metadata":{"id":"1f684bab-1e4e-4251-99b7-839b0b69dbd3"},"outputs":[],"source":["# Input shape\n","embedding_layer_input_shape = (height, width, color_channels)\n","\n","# Output shape\n","embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n","\n","print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n","print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"]},{"cell_type":"markdown","id":"7ee6c9bf-e40f-4511-9325-498251f5b998","metadata":{"id":"7ee6c9bf-e40f-4511-9325-498251f5b998"},"source":["### 4.2 Turning a single image into patches\n","\n","Breaking down the overall architecture into smaller pieces, focusing on the inputs and outputs of individual layers.\n","\n","Let's start with our single image."]},{"cell_type":"code","execution_count":null,"id":"336e1b36-9849-4104-8cb9-bb64a20ffc48","metadata":{"id":"336e1b36-9849-4104-8cb9-bb64a20ffc48"},"outputs":[],"source":["# View single image\n","plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n","plt.title(class_names[label])\n","plt.axis(False);"]},{"cell_type":"markdown","id":"c7ebc6e5-c601-49fa-a185-05a50fdc81cc","metadata":{"id":"c7ebc6e5-c601-49fa-a185-05a50fdc81cc"},"source":["Let's start by just visualizing the top row of patched pixels. We can do this by indexing on the different image dimensions."]},{"cell_type":"code","execution_count":null,"id":"bcd2e784-7989-40e5-b8f5-64de18f1fe3d","metadata":{"id":"bcd2e784-7989-40e5-b8f5-64de18f1fe3d"},"outputs":[],"source":["# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n","image_permuted = image.permute(1, 2, 0)\n","\n","# Index to plot the top row of patched pixels\n","patch_size = 16\n","plt.figure(figsize=(patch_size, patch_size))\n","plt.imshow(image_permuted[:patch_size, :, :]);"]},{"cell_type":"markdown","id":"ad0f2977-7c7b-45e5-91a9-a8626e8e73c7","metadata":{"id":"ad0f2977-7c7b-45e5-91a9-a8626e8e73c7"},"source":["Now we've got the top row, let's turn it into patches."]},{"cell_type":"code","execution_count":null,"id":"93210158-3dcb-4d1f-b728-c7c9c3df99dd","metadata":{"id":"93210158-3dcb-4d1f-b728-c7c9c3df99dd"},"outputs":[],"source":["# Setup hyperparameters and make sure img_size and patch_size are compatible\n","img_size = 224\n","patch_size = 16\n","num_patches = img_size/patch_size\n","assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n","print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n","\n","# Create a series of subplots\n","fig, axs = plt.subplots(nrows=1,\n","                        ncols=img_size // patch_size, # one column for each patch\n","                        figsize=(num_patches, num_patches),\n","                        sharex=True,\n","                        sharey=True)\n","\n","# Iterate through number of patches in the top row\n","for i, patch in enumerate(range(0, img_size, patch_size)):\n","    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n","    axs[i].set_xlabel(i+1) # set the label\n","    axs[i].set_xticks([])\n","    axs[i].set_yticks([])"]},{"cell_type":"markdown","id":"dc30f0a2-7344-4a90-b5b7-7a98127c59fd","metadata":{"id":"dc30f0a2-7344-4a90-b5b7-7a98127c59fd"},"source":["Let's do it for the whole image."]},{"cell_type":"code","execution_count":null,"id":"7d45e15a-eb50-4c46-8055-2acaaacb881c","metadata":{"id":"7d45e15a-eb50-4c46-8055-2acaaacb881c","tags":[]},"outputs":[],"source":["# Setup hyperparameters and make sure img_size and patch_size are compatible\n","img_size = 224\n","patch_size = 16\n","num_patches = img_size/patch_size\n","assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n","print(f\"Number of patches per row: {num_patches}\\\n","        \\nNumber of patches per column: {num_patches}\\\n","        \\nTotal patches: {num_patches*num_patches}\\\n","        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n","\n","# Create a series of subplots\n","fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n","                        ncols=img_size // patch_size,\n","                        figsize=(num_patches, num_patches),\n","                        sharex=True,\n","                        sharey=True)\n","\n","# Loop through height and width of image\n","for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n","    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n","\n","        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n","        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n","                                        patch_width:patch_width+patch_size, # iterate through width\n","                                        :]) # get all color channels\n","\n","        # Set up label information, remove the ticks for clarity and set labels to outside\n","        axs[i, j].set_ylabel(i+1,\n","                             rotation=\"horizontal\",\n","                             horizontalalignment=\"right\",\n","                             verticalalignment=\"center\")\n","        axs[i, j].set_xlabel(j+1)\n","        axs[i, j].set_xticks([])\n","        axs[i, j].set_yticks([])\n","        axs[i, j].label_outer()\n","\n","# Set a super title\n","fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n","plt.show()"]},{"cell_type":"markdown","id":"f774b58d-7095-4272-aba3-fd9a2db4f28f","metadata":{"id":"f774b58d-7095-4272-aba3-fd9a2db4f28f"},"source":["### 4.3 Creating image patches with `torch.nn.Conv2d()`\n","\n","The authors of the ViT paper mention in section 3.1 that the patch embedding is achievable with a convolutional neural network (CNN):  \n","\n","> **Hybrid Architecture.** As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection $\\mathbf{E}$ (Eq. 1) is applied to patches extracted from a **CNN feature map**. As a special case, the patches can have spatial size $1 \\times 1$, which means that the **input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension**. The classification input embedding and position embeddings are added as described above.\n","\n","The \"**feature map**\" they're referring to are the weights/activations produced by a convolutional layer passing over a given image.\n","\n","<img src=\"https://docs.google.com/uc?id=1NrQsAixsulPUcFX4q6MNRyhQx6Or3RvE\" width=900 alt=\"example of creating a patch embedding by passing a convolutional layer over a single image\"/>\n","\n","*By setting the `kernel_size` and `stride` parameters of a [`torch.nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) layer equal to the `patch_size`, we can effectively get a layer that splits our image into patches and creates a learnable embedding (referred to as a \"Linear Projection\" in the ViT paper) of each patch.*\n","\n","* **Input (2D image):** (224, 224, 3) -> (height, width, color channels)\n","* **Output (flattened 2D patches):** (196, 768) -> (number of patches, embedding dimension)\n","\n","We can recreate these with:\n","* [`torch.nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for turning our image into patches of CNN feature maps.\n","* [`torch.nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) for flattening the spatial dimensions of the feature map.\n","\n","Let's start with the `torch.nn.Conv2d()` layer. Replicate the creation of patches by setting the `kernel_size` and `stride` equal to `patch_size`.\n","\n","Set `in_channels=3` for the number of color channels in our image and we'll set `out_channels=768`, the same as the $D$ value in Table 1 for ViT-Base (this is the embedding dimension, each image will be embedded into a learnable vector of size 768)."]},{"cell_type":"code","execution_count":null,"id":"3d4fd046-6b51-4ac0-8d39-e67fb333a18a","metadata":{"id":"3d4fd046-6b51-4ac0-8d39-e67fb333a18a"},"outputs":[],"source":["from torch import nn\n","\n","# Set the patch size\n","patch_size=16\n","\n","# Create the Conv2d layer with hyperparameters from the ViT paper\n","conv2d = nn.Conv2d(in_channels=3, # number of color channels\n","                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n","                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n","                   stride=patch_size,\n","                   padding=0)"]},{"cell_type":"markdown","id":"03dec513-eea5-4d13-b7ab-41c9e997ef48","metadata":{"id":"03dec513-eea5-4d13-b7ab-41c9e997ef48"},"source":["Pass a single image through it."]},{"cell_type":"code","execution_count":null,"id":"1d3424d2-2cfa-431c-9fd0-e2afdf15fc9c","metadata":{"id":"1d3424d2-2cfa-431c-9fd0-e2afdf15fc9c"},"outputs":[],"source":["# View single image\n","plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n","plt.title(class_names[label])\n","plt.axis(False);"]},{"cell_type":"code","execution_count":null,"id":"a72a5614-f1cb-4c01-9696-24f07bf2a219","metadata":{"id":"a72a5614-f1cb-4c01-9696-24f07bf2a219"},"outputs":[],"source":["# Pass the image through the convolutional layer\n","image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n","print(image_out_of_conv.shape)"]},{"cell_type":"markdown","id":"87afeb8f-86cf-4cad-bf88-84dfaccdbe2b","metadata":{"id":"87afeb8f-86cf-4cad-bf88-84dfaccdbe2b"},"source":["Passing our image through the convolutional layer turns it into a series of 768 (this is the embedding size or $D$) feature/activation maps.\n","    \n","```python\n","torch.Size([1, 768, 14, 14]) -> [batch_size, embedding_dim, feature_map_height, feature_map_width]\n","```\n"]},{"cell_type":"code","execution_count":null,"id":"af5b58ca-0d73-4c62-b4af-4e8867b764e2","metadata":{"id":"af5b58ca-0d73-4c62-b4af-4e8867b764e2"},"outputs":[],"source":["# Plot random 5 convolutional feature maps\n","import random\n","random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n","print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n","\n","# Create plot\n","fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n","\n","# Plot random image feature maps\n","for i, idx in enumerate(random_indexes):\n","    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n","    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n","    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"]},{"cell_type":"markdown","id":"b847f2e1-1700-4040-a9aa-df9ab1139cce","metadata":{"id":"b847f2e1-1700-4040-a9aa-df9ab1139cce"},"source":["The important thing to note is that these features may change over time as the neural network learns.\n","\n","And because of these, these feature maps can be considered a **learnable embedding** of our image."]},{"cell_type":"code","execution_count":null,"id":"94f6f5b9-a1c7-4aa1-9780-7cd06457b2b3","metadata":{"id":"94f6f5b9-a1c7-4aa1-9780-7cd06457b2b3"},"outputs":[],"source":["# Get a single feature map in tensor form\n","single_feature_map = image_out_of_conv[:, 0, :, :]\n","single_feature_map, single_feature_map.requires_grad"]},{"cell_type":"markdown","id":"572ae1c5-9488-4882-bdc1-409eef95424e","metadata":{"id":"572ae1c5-9488-4882-bdc1-409eef95424e"},"source":["### 4.4 Flattening the patch embedding with `torch.nn.Flatten()`\n","\n","Let's get them into the desired output shape of the patch embedding layer of the ViT model.\n","\n","* **Desired output (1D sequence of flattened 2D patches):** (196, 768) -> (number of patches, embedding dimension) -> ${N \\times\\left(P^{2} \\cdot C\\right)}$"]},{"cell_type":"markdown","id":"0160c70b-0fe8-42f9-b6e9-5cac23e06836","metadata":{"id":"0160c70b-0fe8-42f9-b6e9-5cac23e06836"},"source":["Reading back through section 3.1 of the ViT paper it says:\n","\n","> As a special case, the patches can have spatial size $1 \\times 1$, which means that the **input sequence is obtained by simply *flattening* the spatial dimensions of the feature map and projecting to the Transformer dimension**.\n","\n","We only want to flatten the \"spatial dimensions of the feature map\". In our case is the `feature_map_height` and `feature_map_width` dimensions of `image_out_of_conv`."]},{"cell_type":"code","execution_count":null,"id":"ed82899d-7bbc-49f9-a423-8fa6344b8e99","metadata":{"id":"ed82899d-7bbc-49f9-a423-8fa6344b8e99"},"outputs":[],"source":["# Create flatten layer\n","flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n","                     end_dim=3) # flatten feature_map_width (dimension 3)"]},{"cell_type":"code","execution_count":null,"id":"e3fa363b-1923-4e27-a0b5-980d885fcda2","metadata":{"id":"e3fa363b-1923-4e27-a0b5-980d885fcda2"},"outputs":[],"source":["# 1. View single image\n","plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n","plt.title(class_names[label])\n","plt.axis(False);\n","print(f\"Original image shape: {image.shape}\")\n","\n","# 2. Turn image into feature maps\n","image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n","print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n","\n","# 3. Flatten the feature maps\n","image_out_of_conv_flattened = flatten(image_out_of_conv)\n","print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"]},{"cell_type":"markdown","id":"fe802095-e944-4607-b3a7-891ba452372b","metadata":{"id":"fe802095-e944-4607-b3a7-891ba452372b"},"source":["It looks like our `image_out_of_conv_flattened` shape is very close to our desired output shape:\n","    \n","* **Desired output (flattened 2D patches):** (196, 768) -> ${N \\times\\left(P^{2} \\cdot C\\right)}$\n","* **Current shape:** (1, 768, 196)\n","\n","The only difference is our current shape has a batch size and the dimensions are in a different order to the desired output."]},{"cell_type":"code","execution_count":null,"id":"47f571a1-2303-4981-85f5-33936b39cf14","metadata":{"id":"47f571a1-2303-4981-85f5-33936b39cf14"},"outputs":[],"source":["# Get flattened image patch embeddings in right shape\n","image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n","print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"]},{"cell_type":"markdown","id":"224d751a-11a0-4645-a225-cd36e507ebf8","metadata":{"id":"224d751a-11a0-4645-a225-cd36e507ebf8"},"source":["We've now matched the desired input and output shapes for the patch embedding layer of the ViT architecture using a couple of PyTorch layers."]},{"cell_type":"markdown","id":"b165987a-8370-471a-a663-711e0c6e60db","metadata":{"id":"b165987a-8370-471a-a663-711e0c6e60db"},"source":["### 4.5 Turning the ViT patch embedding layer into a PyTorch module\n","\n","Create a small PyTorch \"model\" to do all of the steps above."]},{"cell_type":"code","execution_count":null,"id":"3ef75c7e","metadata":{"id":"3ef75c7e"},"outputs":[],"source":["# 1. Create a class which subclasses nn.Module\n","class PatchEmbedding(nn.Module):\n","    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n","\n","    Args:\n","        in_channels (int): Number of color channels for the input images. Defaults to 3.\n","        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n","        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n","    \"\"\"\n","    # 2. Initialize the class with appropriate variables\n","    def __init__(self,\n","                 in_channels:int=3,\n","                 patch_size:int=16,\n","                 embedding_dim:int=768):\n","        super().__init__()\n","\n","        # 3. Create a layer to turn an image into patches\n","        self.patcher = nn.Conv2d(in_channels=in_channels,\n","                                 out_channels=embedding_dim,\n","                                 kernel_size=patch_size,\n","                                 stride=patch_size,\n","                                 padding=0)\n","\n","        # 4. Create a layer to flatten the patch feature maps into a single dimension\n","        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n","                                  end_dim=3)\n","\n","    # 5. Define the forward method\n","    def forward(self, x):\n","        # Create assertion to check that inputs are the correct shape\n","        image_resolution = x.shape[-1]\n","        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n","\n","        # Perform the forward pass\n","        x_patched = self.patcher(x)\n","        x_flattened = self.flatten(x_patched)\n","        # 6. Make sure the output shape has the right order\n","        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"]},{"cell_type":"markdown","id":"5270aa24-85b7-4b5a-a799-8e5eeca47f8f","metadata":{"id":"5270aa24-85b7-4b5a-a799-8e5eeca47f8f"},"source":["Let's try it out on a single image."]},{"cell_type":"code","execution_count":null,"id":"a5599575-44cc-46c9-95a4-e65eb1379a59","metadata":{"id":"a5599575-44cc-46c9-95a4-e65eb1379a59"},"outputs":[],"source":["utils.lab3.set_seeds()\n","\n","# Create an instance of patch embedding layer\n","patchify = PatchEmbedding(in_channels=3,\n","                          patch_size=16,\n","                          embedding_dim=768)\n","\n","# Pass a single image through\n","print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n","patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n","print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"]},{"cell_type":"markdown","id":"a4d59a81-0cef-4251-832b-5f69da199996","metadata":{"id":"a4d59a81-0cef-4251-832b-5f69da199996"},"source":["The output shape matches the ideal input and output shapes we'd like to see from the patch embedding layer:\n","\n","* **Input:** The image starts as 2D with size ${H \\times W \\times C}$.\n","* **Output:** The image gets converted to a 1D sequence of flattened 2D patches with size ${N \\times\\left(P^{2} \\cdot C\\right)}$.\n","\n","Let's now get a summary of our `PatchEmbedding` layer."]},{"cell_type":"code","execution_count":null,"id":"e440be53-d72c-42b8-87c8-1c31c4262c16","metadata":{"id":"e440be53-d72c-42b8-87c8-1c31c4262c16"},"outputs":[],"source":["# Create random input sizes\n","random_input_image = (1, 3, 224, 224)\n","random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n","\n","# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n","# summary(PatchEmbedding(),\n","#         input_size=random_input_image, # try swapping this for \"random_input_image_error\"\n","#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","#         col_width=20,\n","#         row_settings=[\"var_names\"])"]},{"cell_type":"markdown","id":"8576f2f1-a2ad-4874-8f76-08156371f444","metadata":{"id":"8576f2f1-a2ad-4874-8f76-08156371f444"},"source":["### 4.6 Creating the class token embedding\n","\n","Time to get to work on the class token embedding ($\\mathbf{x}_\\text {class }$ from equation 1).\n","\n","<img src=\"https://docs.google.com/uc?id=1PcM4JHnZx2I7F8FPn4m4jtg7zcNF1lzi\" width=900 alt=\"figure 1 from the original vision transformer paper\"/>\n","\n","*Left: Figure 1 from the ViT paper with the \"classification token\" or `[class]` embedding token we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the learnable class embedding token.*\n","\n","Reading the second paragraph of section 3.1 from the ViT paper, we see the following description:\n","\n","> Similar to BERT's `[ class ]` token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4).\n","\n","> **Note:** [BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) is one of the original machine learning research papers to use the Transformer architecture to achieve outstanding results on natural language processing (NLP) tasks and is where the idea of having a `[ class ]` token at the start of a sequence originated, class being a description for the \"classification\" class the sequence belonged to.\n","\n","So we need to \"preprend a learnable embedding to the sequence of embedded patches\"."]},{"cell_type":"code","execution_count":null,"id":"50381789-d73d-4648-9144-4d48da87318f","metadata":{"id":"50381789-d73d-4648-9144-4d48da87318f"},"outputs":[],"source":["print(patch_embedded_image)\n","print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"]},{"cell_type":"markdown","id":"d5e417fc-70c9-43d4-a294-e7728a24bf42","metadata":{"id":"d5e417fc-70c9-43d4-a294-e7728a24bf42"},"source":["To \"prepend a learnable embedding to the sequence of embedded patches\" we need to create a learnable embedding in the shape of the `embedding_dimension` ($D$) and then add it to the `number_of_patches` dimension.\n","\n","We'll get the batch size and embedding dimension shape and then we'll create a `torch.ones()` tensor in the shape `[batch_size, 1, embedding_dimension]`.\n","\n","And we'll make the tensor learnable by passing it to `nn.Parameter()` with `requires_grad=True`."]},{"cell_type":"code","execution_count":null,"id":"cc0bb859-e62e-41a8-9a47-339e4272a152","metadata":{"id":"cc0bb859-e62e-41a8-9a47-339e4272a152"},"outputs":[],"source":["# Get the batch size and embedding dimension\n","batch_size = patch_embedded_image.shape[0]\n","embedding_dimension = patch_embedded_image.shape[-1]\n","\n","# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n","class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n","                           requires_grad=True) # make sure the embedding is learnable\n","\n","# Show the first 10 examples of the class_token\n","print(class_token[:, :, :10])\n","\n","# Print the class_token shape\n","print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"]},{"cell_type":"markdown","id":"f1ce6046-f018-4099-96d1-31dad6fc423b","metadata":{"id":"f1ce6046-f018-4099-96d1-31dad6fc423b"},"source":["> **Note:** Here we're only creating the class token embedding as [`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html) for demonstration purposes, in reality, you'd likely create the class token embedding with [`torch.randn()`](https://pytorch.org/docs/stable/generated/torch.randn.html).\n","\n","The `number_of_tokens` dimension of `class_token` is `1` since we only want to prepend one class token value to the start of the patch embedding sequence.\n","\n","Prepend the token embeddings to our sequence of image patches, `patch_embedded_image` using [`torch.cat()`](https://pytorch.org/docs/stable/generated/torch.cat.html) and set `dim=1`."]},{"cell_type":"code","execution_count":null,"id":"a7287b01-76cb-4371-ab07-981f7bbf2be5","metadata":{"id":"a7287b01-76cb-4371-ab07-981f7bbf2be5"},"outputs":[],"source":["# Add the class token embedding to the front of the patch embedding\n","patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n","                                                      dim=1) # concat on first dimension\n","\n","# Print the sequence of patch embeddings with the prepended class token embedding\n","print(patch_embedded_image_with_class_embedding)\n","print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"]},{"cell_type":"markdown","id":"48502c61-16b0-4659-b95f-0e830ae93077","metadata":{"id":"48502c61-16b0-4659-b95f-0e830ae93077"},"source":["### 4.7 Creating the position embedding\n","\n","Create the position embedding ($\\mathbf{E}_{\\text {pos }}$ from equation 1 where $E$ stands for \"embedding\").\n","\n","<img src=\"https://docs.google.com/uc?id=1qsMq3wI4yW7hgabYa6CZFvYR8JarUNR9\" width=900 alt=\"extracting the position embeddings from the vision transformer architecture and comparing them to other sections of the vision transformer paper\"/>\n","\n","*Left: Figure 1 from the ViT paper with the position embedding we're going to recreate highlighted. Right: Equation 1 and section 3.1 of the ViT paper that relate to the position embedding.*\n","\n","Let's find out more by reading section 3.1 of the ViT paper:\n","\n","> Position embeddings are added to the patch embeddings to retain positional information. We use **standard learnable 1D position embeddings**, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n","\n","By \"retain positional information\" the authors mean they want the architecture to know what \"order\" the patches come in. As in, patch two comes after patch one and patch three comes after patch two and on and on.\n","\n","This positional information can be important when considering what's in an image (without positional information and a flattened sequence could be seen as having no order and thus no patch relates to any other patch)."]},{"cell_type":"markdown","id":"ecd1d068-7cac-46b7-aa43-ea8f01ffed4a","metadata":{"id":"ecd1d068-7cac-46b7-aa43-ea8f01ffed4a"},"source":["Equation 1 states that the position embeddings ($\\mathbf{E}_{\\text {pos }}$) should have the shape $(N + 1) \\times D$:\n","\n","$$\\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}$$\n","\n","Where:\n","* $N=H W / P^{2}$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer (number of patches).\n","* $D$ is the size of the **patch embeddings**, different values for $D$ can be found in Table 1 (embedding dimension).\n","\n","Let's make a learnable 1D embedding with `torch.ones()` to create $\\mathbf{E}_{\\text {pos }}$."]},{"cell_type":"code","execution_count":null,"id":"5bb7f6d1-0824-47eb-a059-6854da5c7433","metadata":{"id":"5bb7f6d1-0824-47eb-a059-6854da5c7433"},"outputs":[],"source":["# Calculate N (number of patches)\n","number_of_patches = int((height * width) / patch_size**2)\n","\n","# Get embedding dimension\n","embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n","\n","# Create the learnable 1D position embedding\n","position_embedding = nn.Parameter(torch.ones(1,\n","                                             number_of_patches+1,\n","                                             embedding_dimension),\n","                                  requires_grad=True) # make sure it's learnable\n","\n","# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n","print(position_embedding[:, :10, :10])\n","print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"]},{"cell_type":"markdown","id":"332facb6-b478-4910-b620-c06d1462d8b8","metadata":{"id":"332facb6-b478-4910-b620-c06d1462d8b8"},"source":["Add them to our sequence of patch embeddings with a prepended class token."]},{"cell_type":"code","execution_count":null,"id":"03370370-e2c2-4e46-bc20-302b97fba9d7","metadata":{"id":"03370370-e2c2-4e46-bc20-302b97fba9d7"},"outputs":[],"source":["# Add the position embedding to the patch and class token embedding\n","patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n","print(patch_and_position_embedding)\n","print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"]},{"cell_type":"markdown","id":"80a39e97-4504-4931-9cec-2569389f3faf","metadata":{"id":"80a39e97-4504-4931-9cec-2569389f3faf"},"source":["Notice how the values of each of the elements in the embedding tensor increases by 1 (this is because of the position embeddings being created with `torch.ones()`).\n","\n","> **Note:** We could put both the class token embedding and position embedding into their own layer if we wanted to. But we'll see later on in section 8 how they can be incorporated into the overall ViT architecture's `forward()` method."]},{"cell_type":"markdown","id":"6654c7ed-eb94-408b-b435-9b352c84328b","metadata":{"id":"6654c7ed-eb94-408b-b435-9b352c84328b"},"source":["### 4.8 Putting it all together: from image to embedding\n","\n","Let's now put everything together in a single code cell and go from input image ($\\mathbf{x}$) to output embedding ($\\mathbf{z}_0$).\n"]},{"cell_type":"code","execution_count":null,"id":"8de90548-e6b0-4123-90ca-a23b0fab52a9","metadata":{"id":"8de90548-e6b0-4123-90ca-a23b0fab52a9"},"outputs":[],"source":["utils.lab3.set_seeds()\n","\n","# 1. Set patch size\n","patch_size = 16\n","\n","# 2. Print shape of original image tensor and get the image dimensions\n","print(f\"Image tensor shape: {image.shape}\")\n","height, width = image.shape[1], image.shape[2]\n","\n","# 3. Get image tensor and add batch dimension\n","x = image.unsqueeze(0)\n","print(f\"Input image with batch dimension shape: {x.shape}\")\n","\n","# 4. Create patch embedding layer\n","patch_embedding_layer = PatchEmbedding(in_channels=3,\n","                                       patch_size=patch_size,\n","                                       embedding_dim=768)\n","\n","# 5. Pass image through patch embedding layer\n","patch_embedding = patch_embedding_layer(x)\n","print(f\"Patching embedding shape: {patch_embedding.shape}\")\n","\n","# 6. Create class token embedding\n","batch_size = patch_embedding.shape[0]\n","embedding_dimension = patch_embedding.shape[-1]\n","class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n","                           requires_grad=True) # make sure it's learnable\n","print(f\"Class token embedding shape: {class_token.shape}\")\n","\n","# 7. Prepend class token embedding to patch embedding\n","patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n","print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n","\n","# 8. Create position embedding\n","number_of_patches = int((height * width) / patch_size**2)\n","position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n","                                  requires_grad=True) # make sure it's learnable\n","\n","# 9. Add position embedding to patch embedding with class token\n","patch_and_position_embedding = patch_embedding_class_token + position_embedding\n","print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"]},{"cell_type":"markdown","id":"02f725de-64d1-41d2-a9d6-374cf6d4f589","metadata":{"id":"02f725de-64d1-41d2-a9d6-374cf6d4f589"},"source":["## 5. Equation 2: Multi-Head Attention (MSA)\n","\n","We'll break down the Transformer Encoder section into two parts (equation 2 and equation 3). Recall equation 2 states:\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L\n","\\end{aligned}\n","$$\n","\n","This indicates a Multi-Head Attention (MSA) layer wrapped in a LayerNorm (LN) layer with a residual connection (the input to the layer gets added to the output of the layer).\n","\n","We'll refer to equation 2 as the \"MSA block\".\n","\n","<img src=\"https://docs.google.com/uc?id=1xBFUVquWxisJ9Z0MdtDdTRfdCqfTaY1A\" alt=\"mapping equation 2 from the ViT paper to the ViT architecture diagram in figure 1\" width=900/>\n","\n","***Left:** Figure 1 from the ViT paper with Multi-Head Attention and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. **Right:** Mapping the Multi-Head Self Attention (MSA) layer, Norm layer and residual connection to their respective parts of equation 2 in the ViT paper.*\n","\n","Many layers you find in research papers are already implemented in modern deep learning frameworks such as PyTorch:\n","* **Multi-Head Self Attention (MSA)** - [`torch.nn.MultiheadAttention()`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html).\n","* **Norm (LN or LayerNorm)** - [`torch.nn.LayerNorm()`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n","* **Residual connection** - add the input to output (we'll see this later on when we create the full Transformer Encoder block in section 7.1)."]},{"cell_type":"markdown","id":"97430d7a-a69b-423c-be2b-ac16e7f9f83f","metadata":{"id":"97430d7a-a69b-423c-be2b-ac16e7f9f83f"},"source":["### 5.1 The LayerNorm (LN) layer\n","\n","[Layer Normalization](https://paperswithcode.com/method/layer-normalization) (`torch.nn.LayerNorm()` or Norm or LayerNorm or LN) normalizes an input over the last dimension.\n","\n","You can find the formal definition of `torch.nn.LayerNorm()` in the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html). PyTorch's `torch.nn.LayerNorm()`'s main parameter is `normalized_shape` which we can set to be equal to the dimension size we'd like to normalize over (in our case it'll be $D$ or `768` for ViT-Base)."]},{"cell_type":"markdown","id":"cf09f6d0-2480-4577-a694-1171898e1777","metadata":{"id":"cf09f6d0-2480-4577-a694-1171898e1777"},"source":["### 5.2 The Multi-Head Self Attention (MSA) layer\n","\n","The power of the self-attention and multi-head attention (self-attention applied multiple times) were revealed in the form of the original Transformer architecture introduced in the [*Attention is all you need*](https://arxiv.org/abs/1706.03762) research paper.\n","\n","Originally designed for text inputs, the original self-attention mechanism takes a sequence of words and then calculates which word should pay more \"attention\" to another word.\n","\n","Since our input is a sequence of image patches rather than words, self-attention and in turn multi-head attention will calculate which patch of an image is most related to another patch, eventually forming a learned representation of an image.\n","\n","You can find the formal definition of the ViT paper's MSA implementation is defined in Appendix A:\n","\n","<img src=\"https://docs.google.com/uc?id=1lqQpKokwQsO1514-QDTH31v5w437Ch42\" alt=\"vision transformer paper figure 1 highlighted with equation 2 and appendix A\" width=900/>\n","\n","***Left:** Vision Transformer architecture overview from Figure 1 of the ViT paper. **Right:** Definitions of equation 2, section 3.1 and Appendix A of the ViT paper highlighted to reflect their respective parts in Figure 1.*\n","\n","The image above highlights the triple embedding input to the MSA layer.\n","\n","This is known as **query, key, value** input or **qkv** for short which is fundamental to the self-attention mechanism. The triple embedding input will be three versions of the output of the Norm layer, one for query, key and value.\n","\n","We can implement the MSA layer in PyTorch with `torch.nn.MultiheadAttention()` with the parameters:\n","* `embed_dim` - the embedding dimension from Table 1 (Hidden size $D$).\n","* `num_heads` - how many attention heads to use (this is where the term \"multihead\" comes from), this value is also in Table 1 (Heads).\n","* `dropout` - whether or not to apply dropout to the attention layer (according to Appendix B.1, dropout isn't used after the qkv-projections).\n","* `batch_first` - does our batch dimension come first? (yes it does)"]},{"cell_type":"markdown","id":"b1a012fa-9bf6-4cf2-bbd0-30ed692f9d74","metadata":{"id":"b1a012fa-9bf6-4cf2-bbd0-30ed692f9d74"},"source":["### 5.3 Replicating Equation 2 with PyTorch layers\n","\n","Let's put everything we've discussed about the LayerNorm (LN) and Multi-Head Attention (MSA) layers in equation 2 into practice.\n"]},{"cell_type":"code","execution_count":null,"id":"b76ae98c","metadata":{"id":"b76ae98c"},"outputs":[],"source":["# 1. Create a class that inherits from nn.Module\n","class MultiheadSelfAttentionBlock(nn.Module):\n","    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n","    \"\"\"\n","    # 2. Initialize the class with hyperparameters from Table 1\n","    def __init__(self,\n","                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n","                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n","                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n","        super().__init__()\n","\n","        # 3. Create the Norm layer (LN)\n","        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n","\n","        # 4. Create the Multi-Head Attention (MSA) layer\n","        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n","                                                    num_heads=num_heads,\n","                                                    dropout=attn_dropout,\n","                                                    batch_first=True) # does our batch dimension come first?\n","\n","    # 5. Create a forward() method to pass the data through the layers\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n","                                             key=x, # key embeddings\n","                                             value=x, # value embeddings\n","                                             need_weights=False) # do we need the weights or just the layer outputs?\n","        return attn_output"]},{"cell_type":"markdown","id":"fc1f0c30-a4ea-41e8-98b2-1a6d8de802d1","metadata":{"id":"fc1f0c30-a4ea-41e8-98b2-1a6d8de802d1"},"source":["> **Note:** Unlike Figure 1, our `MultiheadSelfAttentionBlock` doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell-1}$\" in equation 2), we'll include this when we create the entire Transformer Encoder later on in section 7.1.\n","\n","Let's try it out by create an instance of our `MultiheadSelfAttentionBlock` and passing through the `patch_and_position_embedding` variable we created in section 4.8."]},{"cell_type":"code","execution_count":null,"id":"ceb1dfc0-40ad-4cee-bc54-e9a5cec56895","metadata":{"id":"ceb1dfc0-40ad-4cee-bc54-e9a5cec56895"},"outputs":[],"source":["# Create an instance of MSABlock\n","multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n","                                                             num_heads=12) # from Table 1\n","\n","# Pass patch and position image embedding through MSABlock\n","patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n","print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n","print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"]},{"cell_type":"markdown","id":"5c9f8384-6120-495a-b253-baff5de58097","metadata":{"id":"5c9f8384-6120-495a-b253-baff5de58097"},"source":["Notice how the input and output shape of our data stays the same when it goes through the MSA block."]},{"cell_type":"markdown","id":"236848a7-a1e0-403d-8c78-d1c61d1e45fd","metadata":{"id":"236848a7-a1e0-403d-8c78-d1c61d1e45fd","tags":[]},"source":["## 6. Equation 3: Multilayer Perceptron (MLP)\n","\n","We're on a roll here!\n","\n","Let's keep it going and replicate equation 3:\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L\n","\\end{aligned}\n","$$\n","\n","Here MLP stands for \"multilayer perceptron\" and LN stands for \"layer normalization\" (as discussed above).\n","\n","And the addition on the end is the skip/residual connection.\n","\n","We'll refer to equation 3 as the \"MLP block\" of the Transformer encoder (notice how we're continuing the trend of breaking down the architecture into smaller chunks).\n","\n","<img src=\"https://docs.google.com/uc?id=1e17nMRE25u-BwPmqoB1Oc_rtcQH4IJpW\" alt=\"mapping equation 3 from the ViT paper to the ViT architecture diagram in figure 1\" width=900/>\n","\n","***Left:** Figure 1 from the ViT paper with MLP and Norm layers as well as the residual connection (+) highlighted within the Transformer Encoder block. **Right:** Mapping the multilayer perceptron (MLP) layer, Norm layer (LN) and residual connection to their respective parts of equation 3 in the ViT paper.*"]},{"cell_type":"markdown","id":"589be777-e2af-48d2-9ba7-4cd922aa1528","metadata":{"id":"589be777-e2af-48d2-9ba7-4cd922aa1528","tags":[]},"source":["### 6.1 The MLP layer(s)\n","\n","The term [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) is quite broad as it can refer to almost any combination of *multiple* layers.\n","\n","In the the case of the ViT paper, the MLP structure is defined in section 3.1:\n","\n","> The MLP contains two layers with a GELU non-linearity.\n","\n","Where \"two layers\" refers to linear layers ([`torch.nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) in PyTorch) and \"GELU non-linearity\" is the GELU  (Gaussian Error Linear Units) non-linear activation function ([`torch.nn.GELU()`](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) in PyTorch).\n","\n","Another sneaky detail about the MLP block doesn't appear until Appendix B.1 (Training):\n","\n","> Table 3 summarizes our training setups for our different models. ...Dropout, when used, is applied **after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.**\n","\n","This means that every linear layer (or dense layer) in the MLP block has a dropout layer ([`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) in PyTorch).\n","\n","The value of which can be found in Table 3 of the ViT paper (for ViT-Base, `dropout=0.1`).\n","\n","Knowing this, the structure of our MLP block will be:\n","\n","`layer norm -> linear layer -> non-linear layer -> dropout -> linear layer -> dropout`\n","\n","With hyperparameter values for the linear layers available from Table 1 (MLP size is the number of hidden units between the linear layers and hidden size $D$ is the output size of the MLP block)."]},{"cell_type":"markdown","id":"baebde6a-d03b-4fd1-bb43-dc78454e41e1","metadata":{"id":"baebde6a-d03b-4fd1-bb43-dc78454e41e1","tags":[]},"source":["### 6.2 Replicating Equation 3 with PyTorch layers\n","\n","Let's put everything we've discussed about the LayerNorm (LN) and MLP (MSA) layers in equation 3 into practice."]},{"cell_type":"code","execution_count":null,"id":"68d9dbfe","metadata":{"id":"68d9dbfe"},"outputs":[],"source":["# 1. Create a class that inherits from nn.Module\n","class MLPBlock(nn.Module):\n","    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n","    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n","    def __init__(self,\n","                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n","                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n","                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n","        super().__init__()\n","\n","        # 3. Create the Norm layer (LN)\n","        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n","\n","        # 4. Create the Multilayer perceptron (MLP) layer(s)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_features=embedding_dim,\n","                      out_features=mlp_size),\n","            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n","            nn.Dropout(p=dropout),\n","            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n","                      out_features=embedding_dim), # take back to embedding_dim\n","            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n","        )\n","\n","    # 5. Create a forward() method to pass the data through the layers\n","    def forward(self, x):\n","        x = self.layer_norm(x)\n","        x = self.mlp(x)\n","        return x"]},{"cell_type":"markdown","id":"cfd5dca2-27c7-41dc-a705-d5d82c8d3a39","metadata":{"id":"cfd5dca2-27c7-41dc-a705-d5d82c8d3a39"},"source":["> **Note:** Unlike Figure 1, our `MLPBlock()` doesn't include a skip or residual connection (\"$+\\mathbf{z}_{\\ell}^{\\prime}$\" in equation 3), we'll include this when we create the entire Transformer encoder later on.\n","\n","Create an instance of our `MLPBlock` and passing through the `patched_image_through_msa_block` variable we created in section 5.3."]},{"cell_type":"code","execution_count":null,"id":"442fb987","metadata":{"id":"442fb987"},"outputs":[],"source":["# Create an instance of MLPBlock\n","mlp_block = MLPBlock(embedding_dim=768, # from Table 1\n","                     mlp_size=3072, # from Table 1\n","                     dropout=0.1) # from Table 3\n","\n","# Pass output of MSABlock through MLPBlock\n","patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n","print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n","print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"]},{"cell_type":"markdown","id":"9c4a5b25-c482-4ee1-931a-171559c5f19c","metadata":{"id":"9c4a5b25-c482-4ee1-931a-171559c5f19c"},"source":["Notice how the input and output shape of our data again stays the same when it goes in and out of the MLP block."]},{"cell_type":"markdown","id":"6259bdc6-525a-4bd6-89e8-c2dcdab09d0d","metadata":{"id":"6259bdc6-525a-4bd6-89e8-c2dcdab09d0d"},"source":["## 7. Create the Transformer Encoder\n","\n","Time to stack together our `MultiheadSelfAttentionBlock` (equation 2) and `MLPBlock` (equation 3) and create the Transformer Encoder of the ViT architecture.\n","\n","> The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). **Layernorm (LN) is applied before every block**, and **residual connections after every block** (Wang et al., 2019; Baevski & Auli, 2019).\n","\n","We've created MSA and MLP blocks but what about the residual connections?\n","\n","[Residual connections](https://paperswithcode.com/method/residual-connection) (also called skip connections), were first introduced in the paper [*Deep Residual Learning for Image Recognition*](https://arxiv.org/abs/1512.03385v1) and are achieved by adding a layer(s) input to its subsequent output.\n","\n","In the case of the ViT architecture, the residual connection means the input of the MSA block is added back to the output of the MSA block before it passes to the MLP block. And the same thing happens with the MLP block before it goes onto the next Transformer Encoder block.\n","\n","Or in pseudocode:\n","\n","`x_input -> MSA_block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...`"]},{"cell_type":"markdown","id":"0d8c4f01-e6b5-4b70-b991-a2ec03a6e6b5","metadata":{"id":"0d8c4f01-e6b5-4b70-b991-a2ec03a6e6b5"},"source":["### 7.1 Creating a Transformer Encoder by combining our custom made layers\n","\n","Make a ViT Transformer Encoder with PyTorch by combining our previously created layers."]},{"cell_type":"code","execution_count":null,"id":"2c43855c","metadata":{"id":"2c43855c"},"outputs":[],"source":["# 1. Create a class that inherits from nn.Module\n","class TransformerEncoderBlock(nn.Module):\n","    \"\"\"Creates a Transformer Encoder block.\"\"\"\n","    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n","    def __init__(self,\n","                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n","                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n","                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n","                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n","                 attn_dropout:float=0): # Amount of dropout for attention layers\n","        super().__init__()\n","\n","        # 3. Create MSA block (equation 2)\n","        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n","                                                     num_heads=num_heads,\n","                                                     attn_dropout=attn_dropout)\n","\n","        # 4. Create MLP block (equation 3)\n","        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n","                                   mlp_size=mlp_size,\n","                                   dropout=mlp_dropout)\n","\n","    # 5. Create a forward() method\n","    def forward(self, x):\n","\n","        # 6. Create residual connection for MSA block (add the input to the output)\n","        x =  self.msa_block(x) + x\n","\n","        # 7. Create residual connection for MLP block (add the input to the output)\n","        x = self.mlp_block(x) + x\n","\n","        return x"]},{"cell_type":"markdown","id":"b9199182-d14e-4c54-a391-7a53b6c78a44","metadata":{"id":"b9199182-d14e-4c54-a391-7a53b6c78a44"},"source":["Transformer Encoder block created.\n","\n","<img src=\"https://docs.google.com/uc?id=139Zn_iHXZcG4jCfalfzoiHh9gs5zHiWS\" width=900 alt=\"vision transformer architecture with transformer encoder blocks highlighted\"/>\n","\n","***Left:** Figure 1 from the ViT paper with the Transformer Encoder of the ViT architecture highlighted. **Right:** Transformer Encoder mapped to equation 2 and 3 of the ViT paper, the Transformer Encoder is comprised of alternating blocks of equation 2 (Multi-Head Attention) and equation 3 (Multilayer perceptron).*\n","\n","You might've noticed that Table 1 from the ViT paper has a Layers column. This refers to the number of Transformer Encoder blocks in the specific ViT architecture.\n","\n","In our case, for ViT-Base, we'll be stacking together 12 of these Transformer Encoder blocks to form the backbone of our architecture (we'll get to this in section 8).\n","\n","Let's get a `torchinfo.summary()` of passing an input of shape `(1, 197, 768) -> (batch_size, num_patches, embedding_dimension)` to our Transformer Encoder block."]},{"cell_type":"code","execution_count":null,"id":"a63be4de-ffff-4fa1-97b8-103012797d36","metadata":{"id":"a63be4de-ffff-4fa1-97b8-103012797d36"},"outputs":[],"source":["# Create an instance of TransformerEncoderBlock\n","transformer_encoder_block = TransformerEncoderBlock()\n","\n","# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n","# summary(model=transformer_encoder_block,\n","#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n","#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","#         col_width=20,\n","#         row_settings=[\"var_names\"])"]},{"cell_type":"markdown","id":"8219d44e-0e17-4404-ac0e-45886bfda71e","metadata":{"id":"8219d44e-0e17-4404-ac0e-45886bfda71e","tags":[]},"source":["### 7.2 Creating a Transformer Encoder with PyTorch's Transformer layers\n","\n","Because of their rise in popularity and effectiveness, PyTorch now has in-built [Transformer layers as part of `torch.nn`](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n","\n","For example, we can recreate the `TransformerEncoderBlock` we just created using [`torch.nn.TransformerEncoderLayer()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) and setting the same hyperparameters as above."]},{"cell_type":"code","execution_count":null,"id":"97687357-a884-4653-8302-cb94a1f2c7d3","metadata":{"id":"97687357-a884-4653-8302-cb94a1f2c7d3"},"outputs":[],"source":["# Create the same as above with torch.nn.TransformerEncoderLayer()\n","torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n","                                                             nhead=12, # Heads from Table 1 for ViT-Base\n","                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n","                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n","                                                             activation=\"gelu\", # GELU non-linear activation\n","                                                             batch_first=True, # Do our batches come first?\n","                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n","\n","torch_transformer_encoder_layer"]},{"cell_type":"markdown","id":"4f0a6f44-ac1d-44ac-acf5-b5b47db59402","metadata":{"id":"4f0a6f44-ac1d-44ac-acf5-b5b47db59402"},"source":["To inspect it further, let's get a summary with `torchinfo.summary()`."]},{"cell_type":"code","execution_count":null,"id":"aa4fdb79-d2e8-4b6b-8aec-e9c712089c67","metadata":{"id":"aa4fdb79-d2e8-4b6b-8aec-e9c712089c67"},"outputs":[],"source":["# # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n","# summary(model=torch_transformer_encoder_layer,\n","#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n","#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","#         col_width=20,\n","#         row_settings=[\"var_names\"])"]},{"cell_type":"markdown","id":"aab22163-243b-44e5-a84e-01216b63d9d6","metadata":{"id":"aab22163-243b-44e5-a84e-01216b63d9d6"},"source":["We've replicated a series of equations and layers from a paper, if you need to change the layers and try something different you can.\n","\n","But there are benefits of using the PyTorch pre-built layers, such as:\n","* **Less prone to errors** - Generally, if a layer makes it into the PyTorch standard library, it's been tested and tried to work.\n","* **Potentially better performance** - As of July 2022 and PyTorch 1.12, the PyTorch implemented version of `torch.nn.TransformerEncoderLayer()` can see [a speedup of more than 2x on many common workloads](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/).\n","\n","Finally, since the ViT architecture uses several Transformer Layers stacked on top of each for the full architecture (Table 1 shows 12 Layers in the case of ViT-Base), you can do this with [`torch.nn.TransformerEncoder(encoder_layer, num_layers)`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder) where:\n","* `encoder_layer` - The target Transformer Encoder layer created with `torch.nn.TransformerEncoderLayer()`.\n","* `num_layers` - The number of Transformer Encoder layers to stack together."]},{"cell_type":"markdown","id":"94f9041c-a0af-4fbe-8969-a2bde8637821","metadata":{"id":"94f9041c-a0af-4fbe-8969-a2bde8637821"},"source":["## 8. Putting it all together to create ViT\n","\n","We're going to combine all of the blocks we've created to replicate the full ViT architecture.\n","\n","But, we haven't created equation 4 yet...\n","\n","$$\n","\\begin{aligned}\n","\\mathbf{y} &=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & &\n","\\end{aligned}\n","$$\n","\n","We can put equation 4 into our overall ViT architecture class.\n","\n","All we need is a `torch.nn.LayerNorm()` layer and a `torch.nn.Linear()` layer to convert the 0th index ($\\mathbf{z}_{L}^{0}$) of the Transformer Encoder logit outputs to the target number of classes we have.\n","\n","To create the full architecture, we'll also need to stack a number of our `TransformerEncoderBlock`s on top of each other, we can do this by passing a list of them to `torch.nn.Sequential()` (this will make a sequential range of `TransformerEncoderBlock`s).\n","\n","We'll focus on the ViT-Base hyperparameters from Table 1 but our code should be adaptable to other ViT variants."]},{"cell_type":"code","execution_count":null,"id":"2df890d5","metadata":{"id":"2df890d5"},"outputs":[],"source":["# 1. Create a ViT class that inherits from nn.Module\n","class ViT(nn.Module):\n","    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n","    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n","    def __init__(self,\n","                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n","                 in_channels:int=3, # Number of channels in input image\n","                 patch_size:int=16, # Patch size\n","                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n","                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n","                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n","                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n","                 attn_dropout:float=0, # Dropout for attention projection\n","                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n","                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n","                 num_classes:int=1000): # Default for ImageNet but can customize this\n","        super().__init__() # don't forget the super().__init__()!\n","\n","        # 3. Make the image size is divisible by the patch size\n","        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n","\n","        # 4. Calculate number of patches (height * width/patch^2)\n","        self.num_patches = (img_size * img_size) // patch_size**2\n","\n","        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n","        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n","                                            requires_grad=True)\n","\n","        # 6. Create learnable position embedding\n","        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n","                                               requires_grad=True)\n","\n","        # 7. Create embedding dropout value\n","        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n","\n","        # 8. Create patch embedding layer\n","        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n","                                              patch_size=patch_size,\n","                                              embedding_dim=embedding_dim)\n","\n","        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n","        # Note: The \"*\" means \"all\"\n","        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n","                                                                            num_heads=num_heads,\n","                                                                            mlp_size=mlp_size,\n","                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n","\n","        # 10. Create classifier head\n","        self.classifier = nn.Sequential(\n","            nn.LayerNorm(normalized_shape=embedding_dim),\n","            nn.Linear(in_features=embedding_dim,\n","                      out_features=num_classes)\n","        )\n","\n","    # 11. Create a forward() method\n","    def forward(self, x):\n","\n","        # 12. Get batch size\n","        batch_size = x.shape[0]\n","\n","        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n","        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n","\n","        # 14. Create patch embedding (equation 1)\n","        x = self.patch_embedding(x)\n","\n","        # 15. Concat class embedding and patch embedding (equation 1)\n","        x = torch.cat((class_token, x), dim=1)\n","\n","        # 16. Add position embedding to patch embedding (equation 1)\n","        x = self.position_embedding + x\n","\n","        # 17. Run embedding dropout (Appendix B.1)\n","        x = self.embedding_dropout(x)\n","\n","        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n","        x = self.transformer_encoder(x)\n","\n","        # 19. Put 0 index logit through classifier (equation 4)\n","        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n","\n","        return x"]},{"cell_type":"markdown","id":"b26bd2d5-a7cd-4d14-a2fc-ff8edd8b29b7","metadata":{"id":"b26bd2d5-a7cd-4d14-a2fc-ff8edd8b29b7"},"source":["Let's create a quick demo to showcase what's happening with the class token embedding being expanded over the batch dimensions."]},{"cell_type":"code","execution_count":null,"id":"7dc9f8ec","metadata":{"id":"7dc9f8ec"},"outputs":[],"source":["# Example of creating the class embedding and expanding over a batch dimension\n","batch_size = 32\n","class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n","class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n","\n","# Print out the change in shapes\n","print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n","print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"]},{"cell_type":"markdown","id":"3f07ff70-e09c-4d7c-a119-b2185f46e35a","metadata":{"id":"3f07ff70-e09c-4d7c-a119-b2185f46e35a"},"source":["Notice how the first dimension gets expanded to the batch size and the other dimensions stay the same (because they're inferred by the \"`-1`\" dimensions in `.expand(batch_size, -1, -1)`).\n","\n","Time to test out `ViT()` class.\n","\n","Let's create a random tensor in the same shape as a single image, pass to an instance of `ViT` and see what happens."]},{"cell_type":"code","execution_count":null,"id":"f86190a3-ed1f-4ab3-881c-4eecea996912","metadata":{"id":"f86190a3-ed1f-4ab3-881c-4eecea996912"},"outputs":[],"source":["utils.lab3.set_seeds()\n","\n","# Create a random tensor with same shape as a single image\n","random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n","\n","# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n","vit = ViT(num_classes=len(class_names))\n","\n","# Pass the random image tensor to our ViT instance\n","vit(random_image_tensor)"]},{"cell_type":"markdown","id":"e210b803-f6a4-47e9-af63-dfa92f0eadbe","metadata":{"id":"e210b803-f6a4-47e9-af63-dfa92f0eadbe"},"source":["It looks like our random image tensor made it all the way through our ViT architecture and it's outputting three logit values (one for each class).\n","\n","And because our `ViT` class has plenty of parameters we could customize the `img_size`, `patch_size` or `num_classes` if we wanted to."]},{"cell_type":"markdown","id":"2c0a0c9c-6d98-47fd-a152-8a06be99b5fa","metadata":{"id":"2c0a0c9c-6d98-47fd-a152-8a06be99b5fa"},"source":["### 8.1 Getting a visual summary of our ViT model\n","\n","Let's get a visual overview of the input and output shapes of all the layers in our model.\n","\n","> **Note:** The ViT paper states the use of a batch size of 4096 for training, however, this requires a far bit of CPU/GPU compute memory to handle (the larger the batch size the more memory required). So to make sure we don't get memory errors, we'll stick with a batch size of 32. You could always increase this later if you have access to hardware with more memory."]},{"cell_type":"code","execution_count":null,"id":"494bde26-ed1e-45dc-b615-78ac268ca20e","metadata":{"id":"494bde26-ed1e-45dc-b615-78ac268ca20e"},"outputs":[],"source":["from torchinfo import summary\n","\n","# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n","# summary(model=vit,\n","#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n","#         # col_names=[\"input_size\"], # uncomment for smaller output\n","#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","#         col_width=20,\n","#         row_settings=[\"var_names\"]\n","# )"]},{"cell_type":"markdown","id":"d0279251-5cc1-42a5-bebc-8391ef911343","metadata":{"id":"d0279251-5cc1-42a5-bebc-8391ef911343"},"source":["The total number of parameters too, 85,800,963, our biggest model yet!\n","\n","The number is very close to PyTorch's pretrained ViT-Base with patch size 16 at [`torch.vision.models.vit_b_16()`](https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16) with 86,567,656 total parameters (though this number of parameters is for the 1000 classes in ImageNet).\n","\n","> **Exercise:** Try changing the `num_classes` parameter of our `ViT()` model to 1000 and then creating another summary with `torchinfo.summary()` and see if the number of parameters lines up between our code and `torchvision.models.vit_b_16()`."]},{"cell_type":"markdown","id":"2554a736-ca45-4f83-abc3-41750c20ec58","metadata":{"id":"2554a736-ca45-4f83-abc3-41750c20ec58"},"source":["## 9. Setting up training code for our ViT model\n","### 9.1 Creating an optimizer\n","\n","Searching the ViT paper for \"optimizer\", section 4.1 on Training & Fine-tuning states:\n","\n","> **Training & Fine-tuning.** We train all models, including ResNets, using Adam (Kingma & Ba, 2015 ) with $\\beta_{1}=0.9, \\beta_{2}=0.999$, a batch size of 4096 and apply a high weight decay of $0.1$, which we found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common practices, Adam works slightly better than SGD for ResNets in our setting).\n","\n","The authors set Adam's $\\beta$ (beta) values to $\\beta_{1}=0.9, \\beta_{2}=0.999$, these are the default values for the `betas` parameter in `torch.optim.Adam(betas=(0.9, 0.999))`.\n","\n","They also state the use of [weight decay](https://paperswithcode.com/method/weight-decay) (slowly reducing the values of the weights during optimization to prevent overfitting), we can set this with the `weight_decay` parameter in `torch.optim.Adam(weight_decay=0.3)` (according to the setting of ViT-* trained on ImageNet-1k).\n","\n","We'll set the learning rate of the optimizer to 0.003 as per Table 3 (according to the setting of ViT-* trained on ImageNet-1k).\n","\n","We're going to use a lower batch size than 4096 due to hardware limitations (if you have a large GPU, feel free to increase this)."]},{"cell_type":"markdown","id":"1d434f2f-2723-4ce4-8eac-32a8b9228305","metadata":{"id":"1d434f2f-2723-4ce4-8eac-32a8b9228305"},"source":["### 9.2 Creating a loss function\n","\n","Searching the ViT paper for \"loss\" or \"loss function\" or \"criterion\" returns no results. Since the target problem we're working with is multi-class classification (the same for the ViT paper), we'll use [`torch.nn.CrossEntropyLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."]},{"cell_type":"markdown","id":"73d4f1e2-2922-4b01-9288-e8d206e20dad","metadata":{"id":"73d4f1e2-2922-4b01-9288-e8d206e20dad"},"source":["### 9.3 Training our ViT model\n","\n","Let's setup the training code for training our ViT."]},{"cell_type":"code","execution_count":null,"id":"9107b068-f253-4026-ad21-83be41404043","metadata":{"id":"9107b068-f253-4026-ad21-83be41404043"},"outputs":[],"source":["# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\n","optimizer = torch.optim.Adam(params=vit.parameters(),\n","                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n","                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n","                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n","\n","# Setup the loss function for multi-class classification\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Set the seeds\n","utils.lab3.set_seeds()\n","\n","# Train the model and save the training results to a dictionary\n","results = utils.lab3.train(model=vit.to(device),\n","                       train_dataloader=train_dataloader,\n","                       test_dataloader=test_dataloader,\n","                       optimizer=optimizer,\n","                       loss_fn=loss_fn,\n","                       epochs=10,\n","                       device=device)"]},{"cell_type":"markdown","id":"1d99ae68-6825-42b7-a567-de5701e62014","metadata":{"id":"1d99ae68-6825-42b7-a567-de5701e62014"},"source":["The results on our pizza, steak and sushi dataset don't look too good."]},{"cell_type":"markdown","id":"4ba435de-1521-4d70-a48e-7039062c6a6f","metadata":{"id":"4ba435de-1521-4d70-a48e-7039062c6a6f"},"source":["### 9.4 What our training setup is missing\n","\n","The original ViT architecture achieves good results on several image classification benchmarks (on par or better than many state-of-the-art results when it was released).\n","\n","However, our results (so far) aren't as good.\n","\n","There's a few reasons this could be but the main one is scale.\n","\n","The original ViT paper uses a far larger amount of data than ours (in deep learning, more data is generally always a good thing) and a longer training schedule (see Table 3).\n","\n","| **Hyperparameter value** | **ViT Paper** | **Our implementation** |\n","| ----- | ----- | ----- |\n","| Number of training images | 1.3M (ImageNet-1k), 14M (ImageNet-21k), 303M (JFT) | 225 |\n","| Epochs | 7 (for largest dataset), 90, 300 (for ImageNet) | 10 |\n","| Batch size | 4096 | 32 |\n","| [Learning rate warmup](https://paperswithcode.com/method/linear-warmup) | 10k steps (Table 3) | None |\n","| [Learning rate decay](https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b#:~:text=Learning%20rate%20decay%20is%20a,help%20both%20optimization%20and%20generalization.) | Linear/Cosine (Table 3) | None |\n","| [Gradient clipping](https://paperswithcode.com/method/gradient-clipping) | Global norm 1 (Table 3) | None |\n","\n","Even though our ViT architecture is the same as the paper, the results from the ViT paper were achieved using far more data and a more elaborate training scheme than ours.\n","\n","Because of the size of the ViT architecture and its high number of parameters (increased learning capabilities), and amount of data it uses (increased learning opportunities), many of the techniques used in the ViT paper training scheme such as learning rate warmup, learning rate decay and gradient clipping are specifically designed to [prevent overfitting](https://www.learnpytorch.io/04_pytorch_custom_datasets/#81-how-to-deal-with-overfitting) (regularization).\n","\n","There are many pretrained ViT models (using vast amounts of data) available online, we'll see one in action in section 10."]},{"cell_type":"markdown","id":"32cb3ad2-6eea-4b69-9ce8-322270643919","metadata":{"id":"32cb3ad2-6eea-4b69-9ce8-322270643919"},"source":["### 9.5 Plot the loss curves of our ViT model\n","\n"]},{"cell_type":"code","execution_count":null,"id":"fcca1148-6475-4012-bfc9-cbad2706c22d","metadata":{"id":"fcca1148-6475-4012-bfc9-cbad2706c22d"},"outputs":[],"source":["# Plot our ViT model's loss curves\n","utils.lab3.plot_loss_curves(results)"]},{"cell_type":"markdown","id":"de0f9531-64f3-4e13-8482-ce545d608900","metadata":{"id":"de0f9531-64f3-4e13-8482-ce545d608900"},"source":["## 10. Using a pretrained ViT from `torchvision.models` on the same dataset\n","\n","Reading the ViT paper section 4.2:\n","\n","> Finally, the ViT-L/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in approximately **30 days**.\n","\n","So having a pretrained model available through resources like [`torchvision.models`](https://pytorch.org/vision/stable/models.html), the [`timm` (Torch Image Models) library](https://github.com/rwightman/pytorch-image-models), the [HuggingFace Hub](https://huggingface.co/models) or even from the authors of the papers themselves (many of these resources can be found on [Paperswithcode.com](https://paperswithcode.com/))."]},{"cell_type":"markdown","id":"93027389-1309-47c0-85d3-50e241b617b0","metadata":{"id":"93027389-1309-47c0-85d3-50e241b617b0"},"source":["### 10.1 Getting a pretrained ViT model and creating a feature extractor\n","\n","We can get a pretrained ViT model from `torchvision.models`.\n"]},{"cell_type":"code","execution_count":null,"id":"b8e2dda6-8af0-4255-815f-4d885fa4b477","metadata":{"id":"b8e2dda6-8af0-4255-815f-4d885fa4b477"},"outputs":[],"source":["# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n","utils.lab3.set_seeds()\n","pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n","\n","# Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","print(pretrained_vit_transforms)\n","\n","# pretrained_vit # uncomment for model output"]},{"cell_type":"markdown","id":"182fc970-1650-48b3-914d-0cb3e287beec","metadata":{"id":"182fc970-1650-48b3-914d-0cb3e287beec"},"source":["Pretrained ViT feature extractor model created!\n","\n","Let's now check it out by printing a `torchinfo.summary()`."]},{"cell_type":"code","execution_count":null,"id":"8fbd83a1","metadata":{"id":"8fbd83a1"},"outputs":[],"source":["# # Print a summary using torchinfo (uncomment for actual output)\n","# summary(model=pretrained_vit,\n","#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n","#         # col_names=[\"input_size\"], # uncomment for smaller output\n","#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","#         col_width=20,\n","#         row_settings=[\"var_names\"]\n","# )"]},{"cell_type":"markdown","id":"90c176e5-6453-4911-b8ec-97bab43b437d","metadata":{"id":"90c176e5-6453-4911-b8ec-97bab43b437d"},"source":["The total number of parameters, 85,800,963, is the same as our custom made ViT model above.\n","\n","But the number of trainable parameters for `pretrained_vit` is much, much lower than our custom `vit` at only 2,307 compared to 85,800,963 (in our custom `vit`, since we're training from scratch, all parameters are trainable).\n","\n","This means the pretrained model should train a lot faster, we could potentially even use a larger batch size since less parameter updates are going to be taking up memory."]},{"cell_type":"markdown","id":"76244403-6d3b-472f-a4f0-ccbaa3dfd764","metadata":{"id":"76244403-6d3b-472f-a4f0-ccbaa3dfd764"},"source":["Turn our images into DataLoaders. Since we're using a feature extractor model (less trainable parameters), we could increase the batch size to a higher value (if we set it to 1024, we'd be mimicking an improvement found in [*Better plain ViT baselines for ImageNet-1k*](https://arxiv.org/abs/2205.01580), a paper which improves upon the original ViT paper and suggested extra reading). But since we only have ~200 training samples total, we'll stick with 32."]},{"cell_type":"code","execution_count":null,"id":"dd2f58ff-6182-453a-a802-70ff98c09557","metadata":{"id":"dd2f58ff-6182-453a-a802-70ff98c09557"},"outputs":[],"source":["# Setup dataloaders\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = utils.lab3.create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"]},{"cell_type":"markdown","id":"4e9da731-3c11-4d79-9e68-f006fcedd288","metadata":{"id":"4e9da731-3c11-4d79-9e68-f006fcedd288"},"source":["### 10.2 Train feature extractor ViT model\n","\n","We'll use the Adam optimizer (`torch.optim.Adam()`) with a learning rate of `1e-3` and `torch.nn.CrossEntropyLoss()` as the loss function.\n"]},{"cell_type":"code","execution_count":null,"id":"a49408b4-24d9-4bb1-90a2-dd61c08f78a4","metadata":{"id":"a49408b4-24d9-4bb1-90a2-dd61c08f78a4"},"outputs":[],"source":["# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n","                             lr=1e-3)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","utils.lab3.set_seeds()\n","pretrained_vit_results = utils.lab3.train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=10,\n","                                      device=device)"]},{"cell_type":"markdown","id":"f8309f97-a93c-4975-b837-8387a517b6f4","metadata":{"id":"f8309f97-a93c-4975-b837-8387a517b6f4"},"source":["Our pretrained ViT feature extractor performed far better than our custom ViT model trained from scratch (in the same amount of time)."]},{"cell_type":"markdown","id":"233717e4-9983-47ed-9ef2-5a079df9a971","metadata":{"id":"233717e4-9983-47ed-9ef2-5a079df9a971"},"source":["### 10.3 Plot feature extractor ViT model loss curves\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3c0af18e-6419-4dd6-b8ea-f5830bbd63d5","metadata":{"id":"3c0af18e-6419-4dd6-b8ea-f5830bbd63d5"},"outputs":[],"source":["# Plot the loss curves\n","utils.lab3.plot_loss_curves(pretrained_vit_results)"]},{"cell_type":"markdown","id":"eab07548-3b1c-43a3-9f8d-02672ef1f47c","metadata":{"id":"eab07548-3b1c-43a3-9f8d-02672ef1f47c"},"source":["### 10.6 Save feature extractor ViT model and check file size\n","\n","It looks like our ViT feature extractor model is performing quite well for our Food Vision Mini problem.\n","\n","Perhaps we might want to try deploying it and see how it goes in production (in this case, deploying means putting our trained model in an application someone could use, say taking photos on their smartphone of food and seeing if our model thinks it's pizza, steak or sushi)."]},{"cell_type":"code","execution_count":null,"id":"0fd00943-01aa-4ef4-b366-3cb859a25b6f","metadata":{"id":"0fd00943-01aa-4ef4-b366-3cb859a25b6f"},"outputs":[],"source":["# Save the model\n","utils.lab3.save_model(model=pretrained_vit,\n","                 target_dir=\"models\",\n","                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")"]},{"cell_type":"markdown","id":"0d115e5c-46a0-4063-a3d5-24609f2c9f51","metadata":{"id":"0d115e5c-46a0-4063-a3d5-24609f2c9f51"},"source":["Since we want our Food Vision Mini application to run fast, generally a smaller model with good performance will be better than a larger model with great performance.\n","\n","We can check the size of our model in bytes using the `st_size` attribute of Python's [`pathlib.Path().stat()`](https://docs.python.org/3/library/pathlib.html#pathlib.Path.stat) method whilst passing it our model's filepath name."]},{"cell_type":"code","execution_count":null,"id":"f52ef12c-b88e-4796-84eb-981491a84334","metadata":{"id":"f52ef12c-b88e-4796-84eb-981491a84334"},"outputs":[],"source":["from pathlib import Path\n","\n","# Get the model size in bytes then convert to megabytes\n","pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n","print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")"]},{"cell_type":"markdown","id":"6b63b857-04e1-460c-a510-fc61231b5bc4","metadata":{"id":"6b63b857-04e1-460c-a510-fc61231b5bc4"},"source":["How does this compare to the EffNetB2 feature extractor model from the Part 1?\n","\n","| **Model** | **Model size (MB)** | **Test loss** | **Test accuracy** |\n","| ----- | ----- | ----- | ------ |\n","| EffNetB2 feature extractor^ | 29 | ~0.3906 | ~0.9384 |\n","| ViT feature extractor | 327 | ~0.1084 | ~0.9384 |\n","\n","> **Note:** The EffNetB2 model in reference was trained with 20% of pizza, steak and sushi data (double the amount of images) rather than the ViT feature extractor which was trained with 10% of pizza, steak and sushi data. An exercise would be to train the ViT feature extractor model on the same amount of data and see how much the results improve.\n","\n","The EffNetB2 model is ~11x smaller than the ViT model with similar results for test loss and accuracy.\n","\n","However, the ViT model's results may improve more when trained with the same data (20% pizza, steak and sushi data)."]},{"cell_type":"markdown","id":"04b1569b-117e-43fd-9e0b-324157cb82a4","metadata":{"id":"04b1569b-117e-43fd-9e0b-324157cb82a4"},"source":["## Exercises\n","\n","1. Replicate the ViT architecture we created with in-built [PyTorch transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n","    * You'll want to look into replacing our `TransformerEncoderBlock()` class with [`torch.nn.TransformerEncoderLayer()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) (these contain the same layers as our custom blocks).\n","    * You can stack `torch.nn.TransformerEncoderLayer()`'s on top of each other with [`torch.nn.TransformerEncoder()`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder).\n","2. Turn the custom ViT architecture we created into a Python script, for example, `vit.py`.\n","    * You should be able to import an entire ViT model using something like`from vit import ViT`.\n","3. Train a pretrained ViT feature extractor model on 20% of the pizza, steak and sushi data like the dataset we used in Part 1.\n","    * See how it performs compared to the EffNetB2 model.\n","4. Try repeating the steps from excercise 3 but this time use the \"`ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1`\" pretrained weights from [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16).\n","    * **Note:** ViT pretrained with SWAG weights has a minimum input image size of `(384, 384)` (the pretrained ViT in exercise 3 has a minimum input size of `(224, 224)`), though this is accessible in the weights `.transforms()` method."]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"110a4ad9b3b23ac6a757cfb6c77f1e39e8d0496598f07ec14a944919c025e818"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}